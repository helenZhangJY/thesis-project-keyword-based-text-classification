{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6a0d76-4126-4f66-b3c2-1e9819f7db44",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82519575-c392-4883-8367-67e9ca9064f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0281edb-14ac-4273-9dc8-00f2b9c58479",
   "metadata": {},
   "source": [
    "## evaluate baseline's performance of development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2688c5a-fcf3-4f8a-86c3-b15572f8b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.5634200892334454\n",
      "recall 0.3835616438356164\n",
      "f1-score 0.3788975005940566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/dev_data.csv')\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['bl_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077a9b0-194e-4ee1-bf23-0e7e99935ff8",
   "metadata": {},
   "source": [
    "## evaluate word2vec's performance of development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ab1aa9-b85d-4838-b50d-3ae9d129b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.7079492330348494\n",
      "recall 0.5342465753424658\n",
      "f1-score 0.5205847681089906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/dev_data.csv')\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['w2v_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b099e1-08be-4050-b9c5-04a387c01b2c",
   "metadata": {},
   "source": [
    "## evaluate EDIA system's performance of development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f376045e-d2d9-478a-8739-d35efa936de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.7045059867833842\n",
      "recall 0.4452054794520548\n",
      "f1-score 0.4731526867387673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/dev_data.csv')\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['edia_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6886b5-efd0-46d8-a7c3-f364a66935de",
   "metadata": {},
   "source": [
    "## evaluate baseline's performance of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ddabee-604f-4abb-91da-e4b4c0c7ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.3205099941211052\n",
      "recall 0.2222222222222222\n",
      "f1-score 0.21998824221046445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/test_data.csv',encoding=\"unicode_escape\")\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['bl_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac3b08-2341-4e4f-8f46-76d2a137b664",
   "metadata": {},
   "source": [
    "## evaluate word2vec's performance of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4683226a-360c-4f02-9ab0-88c46a605dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.32782186948853614\n",
      "recall 0.3148148148148148\n",
      "f1-score 0.2878028404344194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/test_data.csv',encoding=\"unicode_escape\")\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['w2v_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eae7a2-e8a8-4ce1-8dd4-a9ddfe6288b8",
   "metadata": {},
   "source": [
    "## evaluate EDIA system's performance of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17b1428d-2cf4-4f58-bac9-95ad023302e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------scores------\n",
      "precision 0.30864197530864196\n",
      "recall 0.2777777777777778\n",
      "f1-score 0.28380230880230883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zjy/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "data = pd.read_csv('../data/test_data.csv',encoding=\"unicode_escape\")\n",
    "y_true = data['gold_label'].to_list()\n",
    "y_pred = data['edia_label'].to_list()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "conf_matrix = pd.DataFrame(cm)\n",
    "\n",
    "print('------scores------')\n",
    "print('precision', precision_score(y_true, y_pred, average='weighted'))\n",
    "print('recall', recall_score(y_true, y_pred, average='weighted'))\n",
    "print('f1-score', f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746eea9-0b11-45f4-ad2b-21dd2dbd8b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
